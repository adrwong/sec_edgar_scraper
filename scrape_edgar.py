from bs4 import BeautifulSoup
import requests
import pymongo
import os
import errno
import time
import re
import unicodedata
import pandas as pd



'''

This programme scrapes the list of stocks(indicated by CIK) in R3000_CIK_list.txt generated by check_stock_list.py
to retrieve all 10-K and 10-Q filings for all companies in the list

'''

# Example link with 10-K s : https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=&owner=exclude&count=100&search_text=
# Example link without 10-K s: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001737336&type=10-K&dateb=&owner=exclude&count=100&search_text=

# Unused tag cleaner

# CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')

# def cleanhtml(raw_html):
#     cleantext = re.sub(CLEANR, '', raw_html)
#     return cleantext

regex = re.compile(r'(>Item(\s|&#160;|&nbsp;)(1A|1B|1|7A|7|8)\.{0,1})|(ITEM\s(1A|1B|1|7A|7|8))')
regexQ = re.compile(r'(>Item(\s|&#160;|&nbsp;)(1A|6)\.{0,1})|(ITEM\s(1A|6))')

def compose_link(cik, doc_type):
    link = "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=" + \
        cik + "&type=" + doc_type + "&dateb=&owner=exclude&count=100&search_text="
    
    return link
    
def pre_clean(text, company_name):
    pre_cleaned = text
    pre_cleaned = re.sub(r'the company', company_name, pre_cleaned, flags=re.IGNORECASE)
    pre_cleaned = re.sub(r'we are', company_name + ' is', pre_cleaned, flags=re.IGNORECASE)
    pre_cleaned = re.sub(r'we', company_name, pre_cleaned, flags=re.IGNORECASE)
    pre_cleaned = re.sub(r'our ', company_name + '\'s ', pre_cleaned, flags=re.IGNORECASE)

    return pre_cleaned
       
def last_clean(text):
    cleaned = text.replace('Table of Contents ', '')
    cleaned = re.sub(r'PART .?.?.? ', '', cleaned)
    cleaned = re.sub(r'Item .?.?.?\.? ', '', cleaned)
    return cleaned

def restore_windows_1252_characters(restore_string):
    """
        Replace C1 control characters in the Unicode string s by the
        characters at the corresponding code points in Windows-1252,
        where possible.
    """

    def to_windows_1252(match):
        try:
            return bytes([ord(match.group(0))]).decode('windows-1252')
        except UnicodeDecodeError:
            # No character at the corresponding code point: remove it.
            return ''
        
    return re.sub(r'[\u0080-\u0099]', to_windows_1252, restore_string)
 
def clean(f, dt, dp):
    # Regex to find <DOCUMENT> tags
    doc_start_pattern = re.compile(r'<DOCUMENT>')
    doc_end_pattern = re.compile(r'</DOCUMENT>')
    # Regex to find <TYPE> tag prceeding any characters, terminating at new line
    type_pattern = re.compile(r'<TYPE>[^\n]+')
    doc_start_is = [x.end() for x in doc_start_pattern.finditer(f)]
    doc_end_is = [x.start() for x in doc_end_pattern.finditer(f)]
    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(f)]
    document = {}

    if dt == '10-K':
        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):
            if doc_type=='10-K':
                document['10-K'] = f[doc_start:doc_end]
        matches = regex.finditer(document['10-K'])
        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])

        test_df.columns = ['item', 'start', 'end']
        test_df['item'] = test_df.item.str.lower()
        test_df.replace('&#160;',' ',regex=True,inplace=True)
        test_df.replace('&nbsp;',' ',regex=True,inplace=True)
        test_df.replace(' ','',regex=True,inplace=True)
        test_df.replace('\.','',regex=True,inplace=True)
        test_df.replace('>','',regex=True,inplace=True)
        pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')
        pos_dat.set_index('item', inplace=True)
        # print(pos_dat)
        # Get Item 1
        item_1_raw = document['10-K'][pos_dat['start'].loc['item1']:pos_dat['start'].loc['item1a']]
        print(BeautifulSoup(item_1_raw, 'lxml').get_text("\n\n"), file=dp)
        # Get Item 1a
        item_1a_raw = document['10-K'][pos_dat['start'].loc['item1a']:pos_dat['start'].loc['item1b']]
        print(BeautifulSoup(item_1a_raw, 'lxml').get_text("\n\n"), file=dp)
        # Get Item 7
        item_7_raw = document['10-K'][pos_dat['start'].loc['item7']:pos_dat['start'].loc['item7a']]
        print(BeautifulSoup(item_7_raw, 'lxml').get_text("\n\n"), file=dp)
        # Get Item 7a
        item_7a_raw = document['10-K'][pos_dat['start'].loc['item7a']:pos_dat['start'].loc['item8']]
        print(BeautifulSoup(item_7a_raw, 'lxml').get_text("\n\n"), file=dp)

    if dt == '10-Q':
        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):
            if doc_type=='10-Q':
                document['10-Q'] = f[doc_start:doc_end]
        matches = regexQ.finditer(document['10-Q'])
        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])

        test_df.columns = ['item', 'start', 'end']
        test_df['item'] = test_df.item.str.lower()
        test_df.replace('&#160;',' ',regex=True,inplace=True)
        test_df.replace('&nbsp;',' ',regex=True,inplace=True)
        test_df.replace(' ','',regex=True,inplace=True)
        test_df.replace('\.','',regex=True,inplace=True)
        test_df.replace('>','',regex=True,inplace=True)
        pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')
        pos_dat.set_index('item', inplace=True)
        # print(pos_dat)
        # Get Item 1a
        item_1a_raw = document['10-Q'][pos_dat['start'].loc['item1a']:pos_dat['end'].loc['item1a']]
        print(BeautifulSoup(item_1a_raw, 'lxml').get_text("\n\n"), file=dp)

    

if __name__ == '__main__':
    
    # myclient = pymongo.MongoClient("mongodb://admin:research21@ec768aa3-bb97-421c-98bd-4822fa7dbea1-0.b8267831955f40ef8fb5530d280e5a10.databases.appdomain.cloud:32481,ec768aa3-bb97-421c-98bd-4822fa7dbea1-1.b8267831955f40ef8fb5530d280e5a10.databases.appdomain.cloud:32481,ec768aa3-bb97-421c-98bd-4822fa7dbea1-2.b8267831955f40ef8fb5530d280e5a10.databases.appdomain.cloud:32481/ibmclouddb?authSource=admin&replicaSet=replset",
    #                                 ssl=True,
    #                                 ssl_ca_certs="../Cloud/5fa7d040-a1ec-11e9-a59f-6a9a0379346c")
    # mydb = myclient["STOCK"]
    # myCollection = mydb.SECEdgarFilings
    
    # # Using GridFS to store extra large (>16Mb) filings
    # fs = gridfs.GridFS(mydb, collection='SECEdgarFilings')
    
    cik_list = open("R3000_CIK_list_edited.txt", 'r').read().splitlines()
    cik_list = list(dict.fromkeys(cik_list))
    doc_types = ["10-K", "10-Q"]
    real_found_list = open("Real_found_list.txt", 'w')
    failed_list = open("failed_log.txt", 'w')
    num_of_operations = len(cik_list) * len(doc_types)
    
    start_time = time.time()
    scrape_log = open('scrape_log.txt', 'w')
    for cik in cik_list:
        for dt in doc_types:
            query = {
                'action': 'getcompany',
                'CIK': cik,
                'type': dt,
                'dateb': '',
                'owner': 'exclude',
                'count': '100',
                'search_text': ''
            }

            headers = {
                'user-agent' : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36"
            }

            url = "https://www.sec.gov/cgi-bin/browse-edgar"
            num_of_operations -= 1
            try:
                response = requests.get(url, params=query, headers=headers)
                html_doc = response.text
                soup = BeautifulSoup(html_doc, 'lxml')
                doc_links = soup.find_all(id="documentsbutton")

                if len(doc_links) == 0:
                    print("CIK: %s, type: %s not found! %s stocks remaining" % (str(cik), dt, str(num_of_operations)))
                    print("CIK: %s, type: %s not found! %s stocks remaining" % (str(cik), dt, str(num_of_operations)),file=scrape_log)
    
                else:
                    for l in doc_links:
                        full_link = "https://www.sec.gov" + l.get('href')
                        success = True
                        try:
                            txt_response = requests.get(full_link, headers=headers)
                            txt_soup = BeautifulSoup(txt_response.text, 'lxml')
                            txt_name = txt_soup.find(text="Complete submission text file").previous_element.next_sibling.next_element.find('a').get('href')
                            txt_link = "https://www.sec.gov" + txt_name

                            data_path = './scraped_data/' + cik + '/' + txt_link[-24:-4]+'.txt' 
                            if not os.path.exists(os.path.dirname(data_path)):
                                try:
                                    os.makedirs(os.path.dirname(data_path))
                                except OSError as exc: #Guard against race condition
                                    if exc.errno != errno.EEXIST:
                                        raise
                                    
                            with open(data_path, 'w', encoding='utf-8') as data:
                                doc_yr = int(txt_link[-24:-4][11:13])
                                try:
                                    content_response = requests.get(txt_link, headers=headers)
                                    content_text = content_response.text
                                    company_name_start = content_text.find("COMPANY CONFORMED NAME:			") + len("COMPANY CONFORMED NAME:			")
                                    company_name_end = content_text.find("CENTRAL INDEX KEY:")
                                    company_name = content_text[company_name_start:company_name_end]
                                    linebreak_pos =  company_name.find('\n')
                                    company_name = company_name[:linebreak_pos]
                                    content_text = pre_clean(content_text, company_name)
                                    clean(content_text, dt, data)
                                        
                                    
                                    
                                    # try:
                                    #     fs.put(''.join(pre_cleaned).encode('utf-8'), company_name = company_name, CIK = int(cik.lstrip('0')) ,\
                                    #     document_type = dt, document_url = txt_link)
                                    # except Exception as e:
                                    #     print("Mongo Exception: " + str(e), file=failed_list)
                                    
                                    
                                except Exception as e:
                                    success = False
                                    print(txt_link + " failed! ", file=failed_list)
                                    print(e, file=failed_list)                          
                        except Exception as e:
                            success = False
                            print(full_link + " failed! ", file=failed_list)
                            print(e, file=failed_list)
                            
                        if (success):
                            print("finished operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations), file=scrape_log)
                            print("finished operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations))

                        else:
                            print("failed operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations), file=scrape_log)
                            print("failed operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations))
                            
            except Exception as e:
                print(str(cik) + " failed! ", file=failed_list)
                print(e, file=failed_list)
        