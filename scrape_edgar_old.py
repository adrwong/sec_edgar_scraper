from bs4 import BeautifulSoup
import requests
import pymongo
import os
import errno
import time
import gridfs
import re
import unicodedata



'''

This programme scrapes the list of stocks(indicated by CIK) in R3000_CIK_list.txt generated by check_stock_list.py
to retrieve all 10-K and 10-Q filings for all companies in the list

'''

# Example link with 10-K s : https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=&owner=exclude&count=100&search_text=
# Example link without 10-K s: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001737336&type=10-K&dateb=&owner=exclude&count=100&search_text=

# Unused tag cleaner

# CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')

# def cleanhtml(raw_html):
#     cleantext = re.sub(CLEANR, '', raw_html)
#     return cleantext

search_dict = {
    'item1' : ['Business', 'Item 1'],
    'item1a' : ['Risk Factors', 'Item 1A'],
    'item7' : ['Managements', 'Discussion', 'Analysis']
}

def timeit(method):
    def timed(*args, **kw):
        ts = time.time()
        result = method(*args, **kw)
        te = time.time()
        if 'log_time' in kw:
            name = kw.get('log_name', method.__name__.upper())
            kw['log_time'][name] = int((te - ts) * 1000)
        else:
            print ('%r  %2.2f ms' % \
                  (method.__name__, (te - ts) * 1000))
        return result
    return timed

def compose_link(cik, doc_type):
    link = "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=" + \
        cik + "&type=" + doc_type + "&dateb=&owner=exclude&count=100&search_text="
    
    return link

def pre_clean(text, company_name):
    pre_cleaned = text
    pre_cleaned = re.sub(r'the company', company_name, pre_cleaned, flags=re.IGNORECASE)
    pre_cleaned = re.sub(r'we are', company_name + ' is', pre_cleaned, flags=re.IGNORECASE)
    pre_cleaned = re.sub(r'we', company_name, pre_cleaned, flags=re.IGNORECASE)
    pre_cleaned = re.sub(r'our ', company_name + '\'s ', pre_cleaned, flags=re.IGNORECASE)

    return pre_cleaned
       
def last_clean(text):
    cleaned = text.replace('Table of Contents ', '')
    cleaned = re.sub(r'PART .?.?.? ', '', cleaned)
    cleaned = re.sub(r'Item .?.?.?\.? ', '', cleaned)
    return cleaned

def restore_windows_1252_characters(restore_string):
    """
        Replace C1 control characters in the Unicode string s by the
        characters at the corresponding code points in Windows-1252,
        where possible.
    """

    def to_windows_1252(match):
        try:
            return bytes([ord(match.group(0))]).decode('windows-1252')
        except UnicodeDecodeError:
            # No character at the corresponding code point: remove it.
            return ''
        
    return re.sub(r'[\u0080-\u0099]', to_windows_1252, restore_string)

def clean(f, dt, dp, company_name):
    all_thematic_break = f
    if dt == '10-K':
        html = f.find('html')
        if html.has_attr('xmlns'):
            # is xml
            all_thematic_break = f.find_all('hr', attrs={'style': 'page-break-after:always'})
        else:
            # is html
            all_thematic_break = f.find_all('hr', attrs={'size':"3", 'style':"COLOR:#999999", 'width':"100%", 'align':"CENTER"})
    elif dt == '10-Q':
        html = f.find('html')
        if html.has_attr('xmlns'):
            # is xml
            all_thematic_break = f.find_all('hr', attrs={'style': 'page-break-after:always'})
        else:
            # is html
            all_thematic_break = f.find_all('hr', attrs={'size':"3", 'style':"COLOR:#999999", 'width':"100%", 'align':"CENTER"})
    
    all_thematic_break = [str(tb) for tb in all_thematic_break]
    f_text = str(f)
    if len(all_thematic_break) > 0:
        regex_delimited_pattern = '|'.join(map(re.escape, all_thematic_break))
        split_filing_string = re.split(regex_delimited_pattern, f_text)
        
    elif len(all_thematic_break) == 0:
        split_filing_string = [f_text]
        
    normalized_text = {}
    
    for index, page in enumerate(split_filing_string):
        page_soup = BeautifulSoup(page, 'html5lib')
        page_text = page_soup.html.body.get_text(' ', strip=True)
        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text))
        page_text_norm = page_text_norm.replace('  ', ' ').replace('\n',' ')
        page_number = index + 1
        normalized_text[page_number] = page_text_norm
        
    matching_words_dict = {}
    
    search_dict_temp = search_dict
    search_dict_temp['company_name'] = [company_name]
    for page_num in normalized_text:
        match = False
        page_text = normalized_text[page_num]
        matching_words_dict[page_num] = {}
        for search_list in search_dict_temp:
            list_of_words = search_dict_temp[search_list]
            for word in list_of_words:
                if word in page_text:
                    match = True
                    break
        if match == True and 'Table of Contents Table of Contents ' not in page_text:
            print(last_clean(page_text), file=dp)

    

if __name__ == '__main__':
    
    # myclient = pymongo.MongoClient("mongodb://admin:research21@ec768aa3-bb97-421c-98bd-4822fa7dbea1-0.b8267831955f40ef8fb5530d280e5a10.databases.appdomain.cloud:32481,ec768aa3-bb97-421c-98bd-4822fa7dbea1-1.b8267831955f40ef8fb5530d280e5a10.databases.appdomain.cloud:32481,ec768aa3-bb97-421c-98bd-4822fa7dbea1-2.b8267831955f40ef8fb5530d280e5a10.databases.appdomain.cloud:32481/ibmclouddb?authSource=admin&replicaSet=replset",
    #                                 ssl=True,
    #                                 ssl_ca_certs="../Cloud/5fa7d040-a1ec-11e9-a59f-6a9a0379346c")
    # mydb = myclient["STOCK"]
    # myCollection = mydb.SECEdgarFilings
    
    # # Using GridFS to store extra large (>16Mb) filings
    # fs = gridfs.GridFS(mydb, collection='SECEdgarFilings')
    
    cik_list = open("R3000_CIK_list_edited.txt", 'r').read().splitlines()
    cik_list = list(dict.fromkeys(cik_list))
    doc_types = ["10-K", "10-Q"]
    real_found_list = open("Real_found_list.txt", 'w')
    failed_list = open("failed_log.txt", 'w')
    num_of_operations = len(cik_list) * len(doc_types)
    
    start_time = time.time()
    scrape_log = open('scrape_log.txt', 'w')
    for cik in cik_list:
        for dt in doc_types:
            query = {
                'action': 'getcompany',
                'CIK': cik,
                'type': dt,
                'dateb': '',
                'owner': 'exclude',
                'count': '100',
                'search_text': ''
            }

            headers = {
                'user-agent' : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36"
            }

            url = "https://www.sec.gov/cgi-bin/browse-edgar"
            num_of_operations -= 1
            try:
                response = requests.get(url, params=query, headers=headers)
                html_doc = response.text
                soup = BeautifulSoup(html_doc, 'lxml')
                doc_links = soup.find_all(id="documentsbutton")

                if len(doc_links) == 0:
                    print("CIK: %s, type: %s not found! %s stocks remaining" % (str(cik), dt, str(num_of_operations)))
                    print("CIK: %s, type: %s not found! %s stocks remaining" % (str(cik), dt, str(num_of_operations)),file=scrape_log)
    
                else:
                    for l in doc_links:
                        full_link = "https://www.sec.gov" + l.get('href')
                        success = True
                        try:
                            txt_response = requests.get(full_link, headers=headers)
                            txt_soup = BeautifulSoup(txt_response.text, 'lxml')
                            txt_name = txt_soup.find(text="Complete submission text file").previous_element.next_sibling.next_element.find('a').get('href')
                            txt_link = "https://www.sec.gov" + txt_name

                            data_path = './scraped_data/' + cik + '/' + txt_link[-24:-4]+'.txt' 
                            if not os.path.exists(os.path.dirname(data_path)):
                                try:
                                    os.makedirs(os.path.dirname(data_path))
                                except OSError as exc: #Guard against race condition
                                    if exc.errno != errno.EEXIST:
                                        raise
                                    
                            with open(data_path, 'w', encoding='utf-8') as data:
                                doc_yr = int(txt_link[-24:-4][11:13])
                                try:
                                    content_response = requests.get(txt_link, headers=headers)
                                    content_text = content_response.text
                                    company_name_start = content_text.find("COMPANY CONFORMED NAME:			") + len("COMPANY CONFORMED NAME:			")
                                    company_name_end = content_text.find("CENTRAL INDEX KEY:")
                                    company_name = content_text[company_name_start:company_name_end]
                                    linebreak_pos =  company_name.find('\n')
                                    company_name = company_name[:linebreak_pos]

                                    fname_pos =  content_text.casefold().find('filename>')+len('filename>')
                                    fname = content_text[fname_pos:].split('\n')[0]
                                    flink = txt_link.split('/')
                                    flink = '/'.join(flink[0:8])
                                    flink = flink+'/'+fname
                                    f = requests.get(flink, headers=headers).text
                                    f = BeautifulSoup(pre_clean(f, company_name), 'lxml')
                                    clean(f, dt, data, company_name)
                                        
                                    
                                    
                                    # try:
                                    #     fs.put(''.join(pre_cleaned).encode('utf-8'), company_name = company_name, CIK = int(cik.lstrip('0')) ,\
                                    #     document_type = dt, document_url = txt_link)
                                    # except Exception as e:
                                    #     print("Mongo Exception: " + str(e), file=failed_list)
                                    
                                    
                                except Exception as e:
                                    success = False
                                    print(txt_link + " failed! ", file=failed_list)
                                    print(e, file=failed_list)
                                    
                        except Exception as e:
                            success = False
                            print(full_link + " failed! ", file=failed_list)
                            print(e, file=failed_list)
                            
                        if (success):
                            print("finished operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations), file=scrape_log)
                            print("finished operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations))
                            start_time=time.time()

                        else:
                            print("failed operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations), file=scrape_log)
                            print("failed operation: %s, type = %s in %s seconds, %d stocks remaining" % (str(cik), dt, str(round(time.time()-start_time, 2)), num_of_operations))
                            start_time=time.time()
                            
            except Exception as e:
                print(str(cik) + " failed! ", file=failed_list)
                print(e, file=failed_list)
        